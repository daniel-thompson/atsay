#!/usr/bin/env python3

#
# atsay
#
# SPDX-License-Identifier: GPL-3.0-or-later
#

import argparse
import array
import numpy as np
import random
import re
import sys
import yaml
import os

from gtts import gTTS as TTS
from pathlib import Path
from pydub import AudioSegment

# It it's installed we'd rather use IPython for interaction...
try:
	import IPython
	interact = IPython.embed
except:
	import pdb
	interact = pdb.set_trace

class AudioBuffer(object):
	"""
	AudioBuffers support far fewer operations than AudioSegments but,
	because the AudioBuffer is a mutable object, those operations can be
	implemented as in-place modification. These in-place operations can be
	more efficient when performing small modifications on large sections of
	audio.
	"""
	def __init__(self, data, *args, **kwargs):
		self.sample_width = kwargs.pop("sample_width", None)
		self.frame_rate = kwargs.pop("frame_rate", None)
		self.channels = kwargs.pop("channels", None)

		if isinstance(data, AudioSegment):
			self.sample_width = data.sample_width
			self.frame_rate = data.frame_rate
			self.channels = data.channels
			data = data.get_array_of_samples()

		data = np.array(data)
		if data.dtype.kind == 'i':
			data = data / float(1 << ((data.dtype.itemsize*8) - 1))
		data = data.reshape(-1, self.channels)
		assert(data.dtype.kind == 'f')

		self._data = data

	def __len__(self):
		"""
		returns the length of this audio segment in milliseconds
		"""
		return round(1000 * (self.frame_count() / self.frame_rate))

	def segment(self):
		"""
		Convert the buffer to an AudioSegment.

		This code would be more efficient if it was migrated into the
		AudioSegment constructor.
		"""

		# Convert from float to int16, flatten the channels and
		# (efficiently) convert to the array form that pydub "likes"
		# to import from.
		flattened = (self._data * 32768).astype(np.int16).reshape((-1,))
		data = array.array('h')
		data.frombytes(flattened.tobytes())

		return AudioSegment(
				data = data,
				sample_width = self.sample_width,
				frame_rate = self.frame_rate,
				channels = self.channels)

	def frame_count(self, ms=None):
		"""
		returns the number of frames for the given number of
		milliseconds, or if not specified, the number of frames
		in the whole AudioSegment
		"""
		if ms is not None:
			return ms * (self.frame_rate / 1000.0)
		else:
			return len(self._data)

	def fade(self, to_gain=0, from_gain=0, start=None, end=None,
		 duration=None):
		# Convert start, end and duration to frame counts and fill
		# in the blanks
		if not start:
			start = end - duration
		if not end:
			end = start + duration
		start = max(int(min(self.frame_count(start),
			            self.frame_count())), 0)
		end = max(int(min(self.frame_count(end),
			          self.frame_count())), 0)
		duration = end - start

		# Apply the fade (if applicable)
		if duration > 0:
			gain_profile = np.logspace(from_gain/20, to_gain/20,
					num=duration) \
				.repeat(self.channels) \
				.reshape(-1, self.channels)
			self._data[start:end] *= gain_profile

		return self

	def fade_out(self, duration):
		return self.fade(to_gain=-120, duration=duration,
				 end=len(self)+1)

	def fade_in(self, duration):
		return self.fade(from_gain=-120, duration=duration, start=0)

	def overlay(self, seg, position=0, gain_during_overlay=None):
		"""
		Overlay the provided segment on to this segment starting at the
		specified position and using the specified looping behaviour.

		seg (AudioSegment or AudioBuffer):
			The audio segment or audio buffer to overlay on to this
			one.

		position (optional int):
			The position to start overlaying the provided segment
			in to this one.

		gain_during_overlay (optional int):
			Changes this segment's volume by the specified amount
			during the duration of time that seg is overlaid on top
			of it. When negative, this has the effect of 'ducking'
			the audio under the overlay.
		"""
		if isinstance(seg, AudioSegment):
			if seg.channels != self.channels:
				seg = seg.set_channels(self.channels)
			if seg.frame_rate != self.frame_rate:
				seg = seg.set_frame_rate(self.frame_rate)

			buf = AudioBuffer(seg)
		else:
			# Audio buffers cannot be resampled... must match
			buf = seg

		start = int(self.frame_count(position))
		end = int(min(start + buf.frame_count(), self.frame_count()))

		if end > start:
			if gain_during_overlay:
				gain = 10 ** (gain_during_overlay / 20)
				self._data[start:end] *= gain

			self._data[start:end] += buf._data[0:end-start]

		return self

class AudioTimetable:
	TEMPLATE = re.compile('[{]([a-zA-Z0-9_]+)[}]')

	def __init__(self, fname):
		self._fname = Path(fname)
		self._cachedir = self._fname.parent / 'atsay-cache'
		self._speech_cache = {}
		self._included = []

		self.dry_run = False
		self.fast = False
		self.include_path = [ self._fname.parent, Path('.') ]

	def export(self, out_f, *args, **kwargs):
		self._parse()
		print(f'>>>            WRITING {out_f}')
		if not self.dry_run:
			self._track.segment().export(out_f, *args, **kwargs)

	@staticmethod
	def _as_time(t):
		ms = t % 1000
		t //= 1000
		s = t % 60
		t //= 60
		m = t % 60
		h = t // 60

		return f'{h}:{m:02}:{s:02}.{ms:03}'

	def _overlay(self, segment, position, gain_during_overlay):
		'''Overlay segment on the current track (with soft-mute)'''
		if self.dry_run:
			return segment

		self._track.overlay(
				segment,
				position=position,
				gain_during_overlay=gain_during_overlay)

		if not self.fast:
			self._track.fade(
					to_gain=gain_during_overlay,
					end=position,
					duration=500)
			self._track.fade(
					from_gain=gain_during_overlay,
					start=position+len(segment),
					duration=500)

	def _say(self, say, lang):
		# Handle {template} substitutions. Substitutions occur
		# at the very last moment before the text is rendered.
		t = self.TEMPLATE.search(say)
		while t:
			full_token = t.group(0)
			token = t.group(1)
			next_token = 'next-' + token

			# Treat any unexpected substitution as a counter
			# that counts upwards from 1
			if token not in self._meta:
				self._meta[token] = 1
				self._meta[next_token] = 1

			say = say.replace(full_token, str(self._meta[token]), 1)
			if next_token in self._meta:
				self._meta[token] += self._meta[next_token]

			t = self.TEMPLATE.search(say)

		print(f'[{self._as_time(self._at)}]  {say}')

		# Automatically take a bigger pause if a
		# saying ends with these characters
		if say[-1] in '.?!':
			self._pause = 500

		# Strip trailing . to improve cache hit rate
		say = say.rstrip('.')

		speech = self._text_to_speech(say, lang)
		self._overlay(
				speech,
				position=self._at,
				gain_during_overlay=-12)

		self._at += len(speech)

	def _play(self, fname):
		play = Path(fname)
		print(f'[{self._as_time(self._at)}]  PLAYING {play}')

		if self.dry_run:
			return

		if not play.exists() and not play.is_absolute():
			play = self._fname.parent / play

		audio = AudioSegment.from_file(play)
		self._overlay(audio,
			      position=self._at,
			      gain_during_overlay=-12)
		self._at += len(audio)

		if self._at > len(self._track):
			self._track.fade_out(duration=5000)

	def _random(self, t):
		least_visited = max(self._counts.values())
		for id in t['random']:
			if id not in self._counts:
				self._counts[id] = 0
			if self._counts[id] < least_visited:
				least_visited = self._counts[id]

		candidates = [ id for id in t['random'] if
				self._counts[id] <= least_visited ]
		id = random.choice(candidates)

		self._counts[id] += 1

		# Update the parental counts (unless it has been inhibited)
		parent = self._random_parent[-1]
		if parent:
			if len(candidates) > 1:
				self._counts[parent] = least_visited
			else:
				self._counts[parent] = least_visited + 1
		self._random_parent.append(id
				if 'no-inherit' not in t else None)

		return id

	def _render(self, timetable, position=0, speed=1):
		for t in timetable:
			self._pause = 0

			if 'at' in t:
				self._at = position + int(t['at'] / speed)

			# Store the original at (in case we get a length
			# command)
			originally_at = self._at

			if 'repeat' in t:
				newt = dict(t)
				newt.pop('repeat')
				newt.pop('at', None)
				for i in range(t['repeat']):
					self._render([ newt ], self._at)
				continue

			if 'say' in t:
				self._say(t['say'], t['lang'])

			if 'translate' in t:
				self._at += 500
				self._say(t['say'], t['lang'])
				self._at += 500
				self._say(t['translate'], t['to'])
				self._at += 500
				self._say(t['say'], t['lang'])

			if 'play' in t:
				self._play(t['play'])

			if 'pause' in t:
				self._pause = int(t['pause'])

			if 'macro' in t:
				id = t['macro']
				newspeed = t['speed'] if 'speed' in t else 1
				self._render(self._macros[id], self._at, newspeed * speed)

			if 'random' in t:
				id = self._random(t)
				newspeed = t['speed'] if 'speed' in t else 1
				self._render(self._macros[id], self._at, newspeed * speed)
				self._random_parent.pop()

			self._at += self._pause

			if 'length' in t:
				self._at = originally_at + t['length']


		self._pause = 0

	def _include(self, include_fname):
		'''Parses meta/include, meta/lang, meta/translate and macros'''
		if include_fname in self._included:
			return
		else:
			self._included.append(include_fname)
		if include_fname:
			for p in self.include_path:
				fname = p / include_fname
				if fname.exists():
					break
			print(f'>>>            INCLUDING {fname}')
			with open(fname) as f:
				y = yaml.safe_load(f)

		else:
			y = self._raw

		if 'meta' in y and 'include' in y['meta']:
			for i in y['meta']['include']:
				self._include(i)

		if 'meta' in y and 'lang' in y['meta']:
			lang = y['meta']['lang']
		else:
			lang = 'en'

		if 'meta' in y and 'translate' in y['meta']:
			translate = y['meta']['translate']
		else:
			translate = 'en'

		def add_inherited_tags(t):
			if 'say' in t and 'lang' not in t:
				# TODO: Syntax errors in macros (no list after
				#       label) cause this assignment to fail.
				t['lang'] = lang
			if 'translate' in t and 'to' not in t:
				t['to'] = translate

		if 'timetable' in y:
			for t in y['timetable']:
				add_inherited_tags(t)

		if 'macros' in y:
			for k, v in y['macros'].items():
				for t in v:
					add_inherited_tags(t)
				self._macros[k] = v

	def _parse(self):
		def parse_time(t):
			# YAML library means that h:mm:ss comes to us as
			# a string but h:mm:ss.ms comes to us a float!
			if isinstance(t, float):
				return int(t * 1000)

			hms = t
			ms = 0

			hms_ms = t.split('.', 1)
			if len(hms_ms) == 2:
				hms = hms_ms[0]
				ms = int(hms_ms[1])

			hms = hms.split(':', 2)
			h = int(hms[-3]) if len(hms) >= 3 else 0
			m = int(hms[-2]) if len(hms) >= 2 else 0
			s = int(hms[-1])

			m += h * 60
			s += m * 60
			ms += s * 1000

			return ms

		with open(str(self._fname)) as f:
			self._raw = yaml.safe_load(f)

		self._counts = { None : 0 }
		self._random_parent = [ None ]

		self._meta = self._raw['meta']
		self._timetable = self._raw['timetable']

		self._macros = {}
		self._include(None)

		self._meta['length'] = parse_time(self._meta['length'])

		def force_types(t):
			if 'at' in t:
				t['at'] = parse_time(t['at'])
			if 'length' in t:
				t['length'] = parse_time(t['length'])
			if 'say' in t:
				t['say'] = str(t['say'])

		for t in self._timetable:
			force_types(t)

		for macro in self._macros.values():
			for t in macro:
				force_types(t)

		self._at = 0

		# Create a silent stereo track of the right duration
		segment = AudioSegment.silent(
				duration=self._meta['length'],
				frame_rate=44100).set_channels(2)
		self._track = AudioBuffer(segment)

		self._render(self._timetable)

		assert(1 == len(self._random_parent))

	def _text_to_speech(self, say, lang='en'):
		if self.dry_run:
			return AudioSegment.silent(
					duration=1900,
					frame_rate=44100).set_channels(2)

		if say not in self._speech_cache:
			if not self._cachedir.exists():
				os.mkdir(self._cachedir)

			cachefile = self._cachedir / f'{say}-{lang}.mp3'
			if not cachefile.exists():
				TTS(say, lang=lang).save(cachefile)

			self._speech_cache[say] = AudioSegment.from_mp3(
					str(cachefile))

		return self._speech_cache[say]

def main():
	parser = argparse.ArgumentParser()
	parser.add_argument('infile',
			help='input filename')
	parser.add_argument('-D', '--define', action='append', default=[],
			help='add meta-data from the command line')
	parser.add_argument('--dry-run', action='store_true',
			help='fast syntax check')
	parser.add_argument('--fast', action='store_true',
			help='enable fast-mode (a.k.a. disable soft-muting)')
	parser.add_argument('-I', '--include', action='append', default=[],
			help='extend the search path')
	parser.add_argument('-o', '--output', dest='outfile',
			help='output filename')
	args = parser.parse_args()

	infile = Path(args.infile)
	outfile = infile.parent / (infile.stem + '.wav')
	if args.outfile:
		outfile = Path(args.outfile)
	fmt = outfile.suffix.lstrip('.')

	tags = {
		'artist': 'Audio Timetable',
	}

	atsay = AudioTimetable(infile)
	atsay.dry_run = args.dry_run
	atsay.fast = args.fast
	atsay.include_path = [ Path(p) for p in args.include ] + atsay.include_path
	def pair(s):
		l = s.split('=', 1)
		if len(l) == 1:
			l.append(None)
		return l
	atsay.defines = { k: v for k, v in [ pair(s) for s in args.define ] }

	random.seed()
	atsay.export(outfile, format=fmt, tags=tags)

main()
