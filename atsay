#!/usr/bin/env python3

#
# atsay
#
# SPDX-License-Identifier: GPL-3.0-or-later
#

import argparse
import random
import re
import sys
import yaml
import os

from gtts import gTTS as TTS
from pathlib import Path
from pydub import AudioSegment

# It it's installed we'd rather use IPython for interaction...
try:
	import IPython
	interact = IPython.embed
except:
	import pdb
	interact = pdb.set_trace

class AudioTimetable:
	TEMPLATE = re.compile('[{]([a-zA-Z0-9_]+)[}]')

	def __init__(self, fname):
		self._fname = Path(fname)
		self._cachedir = self._fname.parent / 'atsay-cache'

		self.fast = False
		self.dry_run = False

	def export(self, *args, **kwargs):
		self._parse()
		if not self.dry_run:
			self._track.export(*args, **kwargs)

	@staticmethod
	def _as_time(t):
		ms = t % 1000
		t //= 1000
		s = t % 60
		t //= 60
		m = t % 60
		h = t // 60

		return f'{h}:{m:02}:{s:02}.{ms:03}'

	def _overlay(self, segment, position, gain_during_overlay):
		'''Overlay segment on the current track (with soft-mute)'''
		if self.dry_run:
			return segment

		middle_start = position
		middle_end = position + len(segment)
		duration = 500

		beginning = self._track[:middle_start]
		middle = self._track[middle_start:middle_end]
		end = self._track[middle_end:]

		middle = middle.overlay(
				segment,
				position=0,
				gain_during_overlay=gain_during_overlay)
		if not self.fast:
			beginning = beginning.fade(
					to_gain=gain_during_overlay,
					duration=duration,
					end=float('inf'))
			end = end.fade(
					from_gain=gain_during_overlay,
					duration=duration,
					start=0)

		return beginning + middle + end

	def _say(self, say, lang):
		# Handle {template} substitutions. Substitutions occur
		# at the very last moment before the text is rendered.
		t = self.TEMPLATE.search(say)
		while t:
			full_token = t.group(0)
			token = t.group(1)
			next_token = 'next-' + token

			# Treat any unexpected substitution as a counter
			# that counts upwards from 1
			if token not in self._meta:
				self._meta[token] = 1
				self._meta[next_token] = 1

			say = say.replace(full_token, str(self._meta[token]), 1)
			if next_token in self._meta:
				self._meta[token] += self._meta[next_token]

			t = self.TEMPLATE.search(say)

		print(f'[{self._as_time(self._at)}]  {say}')

		# Automatically take a bigger pause if a
		# saying ends with these characters
		if say[-1] in '.?!':
			self._pause = 500

		# Strip trailing . to improve cache hit rate
		say = say.rstrip('.')

		speech = self._text_to_speech(say, lang)
		self._track = self._overlay(
				speech,
				position=self._at,
				gain_during_overlay=-12)

		self._at += len(speech)

	def _play(self, fname):
		play = Path(fname)
		print(f'[{self._as_time(self._at)}]  PLAYING {play}')

		if self.dry_run:
			return

		if not play.exists() and not play.is_absolute():
			play = self._fname.parent / play

		audio = AudioSegment.from_file(play)
		self._track = self._overlay(
				audio,
				position=self._at,
				gain_during_overlay=-12)
		self._at += len(audio)

		if self._at > len(self._track):
			self._track = self._track.fade_out(duration=5000)

	def _render(self, timetable, position=0, speed=1):
		for t in timetable:
			self._pause = 0

			if 'at' in t:
				self._at = position + int(t['at'] / speed)

			if 'repeat' in t:
				newt = dict(t)
				newt.pop('repeat')
				newt.pop('at', None)
				for i in range(t['repeat']):
					self._render([ newt ], self._at)
				continue

			if 'say' in t:
				self._say(t['say'], t['lang'])

			if 'translate' in t:
				self._at += 500
				self._say(t['say'], t['lang'])
				self._at += 500
				self._say(t['translate'], t['to'])
				self._at += 500
				self._say(t['say'], t['lang'])

			if 'play' in t:
				self._play(t['play'])

			if 'pause' in t:
				self._pause = int(t['pause'])

			if 'macro' in t:
				id = t['macro']
				newspeed = t['speed'] if 'speed' in t else 1
				self._render(self._macros[id], self._at, newspeed * speed)

			if 'random' in t:
				if 'shuffle' not in t or not len(t['shuffle']):
					t['shuffle'] = list(t['random'])
					random.shuffle(t['shuffle'])
				id = t['shuffle'].pop()
				newspeed = t['speed'] if 'speed' in t else 1
				self._render(self._macros[id], self._at, newspeed * speed)

			self._at += self._pause

		self._pause = 0

	def _include(self, fname):
		'''Parses meta/include, meta/lang, meta/translate and macros'''
		if fname:
			print(f'>>>            INCLUDING {fname}')
			with open(str(self._fname.parent / fname)) as f:
				y = yaml.safe_load(f)
		else:
			y = self._raw

		if 'meta' in y and 'include' in y['meta']:
			for i in y['meta']['include']:
				self._include(i)

		if 'meta' in y and 'lang' in y['meta']:
			lang = y['meta']['lang']
		else:
			lang = 'en'

		if 'meta' in y and 'translate' in y['meta']:
			translate = y['meta']['translate']
		else:
			translate = 'en'

		def add_inherited_tags(t):
			if 'say' in t and 'lang' not in t:
				t['lang'] = lang
			if 'translate' in t and 'to' not in t:
				t['to'] = translate

		if 'timetable' in y:
			for t in y['timetable']:
				add_inherited_tags(t)

		if 'macros' in y:
			for k, v in y['macros'].items():
				for t in v:
					add_inherited_tags(t)
				self._macros[k] = v

	def _parse(self):
		def parse_time(t):
			hms = t
			ms = 0

			hms_ms = t.split('.', 1)
			if len(hms_ms) == 2:
				hms = hms_ms[0]
				ms = int(hms_ms[1])

			hms = hms.split(':', 2)
			h = int(hms[-3]) if len(hms) >= 3 else 0
			m = int(hms[-2]) if len(hms) >= 2 else 0
			s = int(hms[-1])

			m += h * 60
			s += m * 60
			ms += s * 1000

			return ms

		with open(str(self._fname)) as f:
			self._raw = yaml.safe_load(f)

		self._meta = self._raw['meta']
		self._timetable = self._raw['timetable']

		self._macros = {}
		self._include(None)

		self._meta['length'] = parse_time(self._meta['length'])

		def force_types(t):
			if 'at' in t:
				t['at'] = parse_time(t['at'])
			if 'say' in t:
				t['say'] = str(t['say'])

		for t in self._timetable:
			force_types(t)

		for macro in self._macros.values():
			for t in macro:
				force_types(t)

		self._at = 0

		# Create a silent stereo track of the right duration
		self._track = AudioSegment.silent(
				duration=self._meta['length'],
				frame_rate=44100).set_channels(2)

		self._render(self._timetable)

	def _text_to_speech(self, say, lang='en'):
		if self.dry_run:
			return AudioSegment.silent(
					duration=1900,
					frame_rate=44100).set_channels(2)
		if not self._cachedir.exists():
			os.mkdir(self._cachedir)

		cacheline = self._cachedir / f'{say}-{lang}.mp3'
		if not cacheline.exists():
			TTS(say, lang=lang).save(cacheline)

		return AudioSegment.from_mp3(str(cacheline))

def main():
	parser = argparse.ArgumentParser()
	parser.add_argument('infile')
	parser.add_argument('--dry-run', action='store_true',
			help='fast syntax check')
	parser.add_argument('--fast', action='store_true',
			help='enable fast-mode (a.k.a. disable soft-muting)')
	args = parser.parse_args()

	infile = Path(args.infile)
	outfile = infile.parent / (infile.stem + '.wav')

	atsay = AudioTimetable(infile)
	atsay.dry_run = args.dry_run
	atsay.fast = args.fast

	random.seed()
	atsay.export(outfile, format='wav')

main()
