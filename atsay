#!/usr/bin/env python3

#
# atsay
#
# SPDX-License-Identifier: GPL-3.0-or-later
#

import sys
import yaml
import os

from gtts import gTTS as TTS
from pathlib import Path
from pydub import AudioSegment

# It it's installed we'd rather use IPython for interaction...
try:
	import IPython
	interact = IPython.embed
except:
	import pdb
	interact = pdb.set_trace

class AudioTimetable:
	def __init__(self, fname):
		self._fname = Path(fname)
		self._cachedir = self._fname.parent / 'atsay-cache'

		self._parse()
		self._render()

	def export(self, *args, **kwargs):
		self._track.export(*args, **kwargs)

	def _overlay(self, timetable, position=0):
		def as_time(t):
			ms = t % 1000
			t //= 1000
			s = t % 60
			t //= 60
			m = t % 60
			h = t // 60

			return f'{h}:{m:02}:{s:02}.{ms:03}'

		def overlay(segment, position, gain_during_overlay):
			middle_start = position
			middle_end = position + len(segment)
			duration = 500

			beginning = self._track[:middle_start]
			middle = self._track[middle_start:middle_end]
			end = self._track[middle_end:]

			beginning = beginning.fade(
					to_gain=gain_during_overlay,
					duration=duration,
					end=float('inf'))
			middle = middle.overlay(
					segment,
					position=0,
					gain_during_overlay=gain_during_overlay)
			end = end.fade(
					from_gain=gain_during_overlay,
					duration=duration,
					start=0)

			return beginning + middle + end

		for t in timetable:
			if 'at' in t:
				self._at = position + t['at']

			if 'say' in t:
				say = t['say']
				print(f'[{as_time(self._at)}]  {say}')

				speech = self._text_to_speech(say)
				self._track = overlay(
						speech,
						position=self._at,
						gain_during_overlay=-12)
				self._at += len(speech)

			if 'play' in t:
				play = Path(t['play'])
				print(f'[{as_time(self._at)}]  PLAYING {play}')

				if not play.exists() and not play.is_absolute():
					play = self._fname.parent / play

				audio = AudioSegment.from_file(play)
				self._track = overlay(
						audio,
						position=self._at,
						gain_during_overlay=-12)
				self._at += len(audio)

			if 'macro' in t:
				id = t['macro']
				self._overlay(self._macros[id], self._at)

	def _include(self, fname):
		'''Parses meta/include and macros'''
		if fname:
			print(f'>>>            INCLUDING {fname}')
			with open(str(self._fname.parent / fname)) as f:
				y = yaml.safe_load(f)
		else:
			y = self._raw

		if 'meta' in y and 'include' in y['meta']:
			for i in y['meta']['include']:
				self._include(i)

		if 'macros' in y:
			for k, v in y['macros'].items():
				self._macros[k] = v

	def _parse(self):
		def parse_time(t):
			hms = t
			ms = 0

			hms_ms = t.split('.', 1)
			if len(hms_ms) == 2:
				hms = hms_ms[0]
				ms = int(hms_ms[1])

			hms = hms.split(':', 2)
			h = int(hms[-3]) if len(hms) >= 3 else 0
			m = int(hms[-2]) if len(hms) >= 2 else 0
			s = int(hms[-1])

			m += h * 60
			s += m * 60
			ms += s * 1000

			return ms

		with open(str(self._fname)) as f:
			self._raw = yaml.safe_load(f)

		self._meta = self._raw['meta']
		self._timetable = self._raw['timetable']

		self._macros = {}
		self._include(None)

		self._meta['length'] = parse_time(self._meta['length'])

		for t in self._timetable:
			if 'at' in t:
				t['at'] = parse_time(t['at'])

		for macro in self._macros.values():
			for t in macro:
				t['at'] = parse_time(t['at'])

	def _render(self):
		self._at = 0

		# Create a silent stereo track of the right duration
		self._track = AudioSegment.silent(
				duration=self._meta['length'],
				frame_rate=44100).set_channels(2)

		self._overlay(self._timetable)

	def _text_to_speech(self, say):
		if not self._cachedir.exists():
			os.mkdir(self._cachedir)

		cacheline = self._cachedir / f'{say}.mp3'
		if not cacheline.exists():
			TTS(say).save(cacheline)

		return AudioSegment.from_mp3(str(cacheline))

fname = Path(sys.argv[1])
atsay = AudioTimetable(fname)
atsay.export(fname.stem + '.wav', format='wav')
