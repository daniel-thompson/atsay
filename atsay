#!/usr/bin/env python3

#
# atsay
#
# SPDX-License-Identifier: GPL-3.0-or-later
#

import sys
import yaml
import os

from gtts import gTTS as TTS
from pathlib import Path
from pydub import AudioSegment

# It it's installed we'd rather use IPython for interaction...
try:
	import IPython
	interact = IPython.embed
except:
	import pdb
	interact = pdb.set_trace

class AudioTimetable:
	def __init__(self, fname):
		self._fname = Path(fname)
		self._cachedir = self._fname.parent / 'atsay-cache'

		with open(self._fname) as f:
			self._raw = yaml.safe_load(f)
		self._parse()
		self._render()

	def export(self, *args, **kwargs):
		self._track.export(*args, **kwargs)

	def _overlay(self, timetable, position=0):
		def as_time(t):
			ms = t % 1000
			t //= 1000
			s = t % 60
			t //= 60
			m = t % 60
			h = t // 60

			return f'{h}:{m:02}:{s:02}.{ms:03}'

		for t in timetable:
			if 'at' in t:
				self._at = position + t['at']

			if 'say' in t:
				say = t['say']
				print(f'[{as_time(self._at)}]  {say}')

				speech = self._text_to_speech(say)
				self._track = self._track.overlay(
						speech,
						position=self._at,
						gain_during_overlay=-12)
				self._at += len(speech)

			if 'play' in t:
				play = t['play']
				print(f'[{as_time(self._at)}]  PLAYING {play}')

				audio = AudioSegment.from_file(play)
				self._track = overlay(
						audio,
						position=self._at,
						gain_during_overlay=-12)
				self._at += len(audio)

			if 'macro' in t:
				id = t['macro']
				self._overlay(self._macros[id], self._at)


	def _parse(self):
		def parse_time(t):
			hms = t
			ms = 0

			hms_ms = t.split('.', 1)
			if len(hms_ms) == 2:
				hms = hms_ms[0]
				ms = int(hms_ms[1])

			hms = hms.split(':', 2)
			h = int(hms[-3]) if len(hms) >= 3 else 0
			m = int(hms[-2]) if len(hms) >= 2 else 0
			s = int(hms[-1])

			m += h * 60
			s += m * 60
			ms += s * 1000

			return ms

		self._meta = self._raw['meta']
		if 'macros' in self._raw:
			self._macros = self._raw['macros']
		else:
			self._macros = {}
		self._timetable = self._raw['timetable']

		self._meta['length'] = parse_time(self._meta['length'])

		for t in self._timetable:
			if 'at' in t:
				t['at'] = parse_time(t['at'])

		for macro in self._macros.values():
			for t in macro:
				t['at'] = parse_time(t['at'])

	def _render(self):
		self._at = 0

		# Create a silent stereo track of the right duration
		self._track = AudioSegment.silent(
				duration=self._meta['length'],
				frame_rate=44100).set_channels(2)

		self._overlay(self._timetable)

	def _text_to_speech(self, say):
		if not self._cachedir.exists():
			os.mkdir(self._cachedir)

		cacheline = self._cachedir / f'{say}.mp3'
		if not cacheline.exists():
			TTS(say).save(cacheline)

		return AudioSegment.from_mp3(cacheline)

fname = Path(sys.argv[1])
atsay = AudioTimetable(fname)
atsay.export(fname.stem + '.wav', format='wav')
